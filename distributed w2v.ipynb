{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviewsand 50000 unlabeled reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data from files\n",
    "train = pd.read_csv(\"../word2vec-nlp-tutorial/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv(\"../word2vec-nlp-tutorial/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "unlabeled_train = pd.read_csv(\"../word2vec-nlp-tutorial/unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "#Verify the number of reviews that were read (100,000 in total)\n",
    "print(\"Read %d labeled train reviews, %d labeled test reviews\" \\\n",
    "      \"and %d unlabeled reviews\\n\" % (train[\"review\"].size, test[\"review\"].size, unlabeled_train[\"review\"].size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import various modules for string cleaning \n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_wordlist(review, remove_stopwords=False, remove_number=False):\n",
    "    # Function to convert a document to a sequence of words, optionally removing stop words & numbers.\n",
    "    # Return a list of words\n",
    "    \n",
    "    #1. remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    \n",
    "    #2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z0-9]\", \" \", review_text)\n",
    "    \n",
    "    #3. Covert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    \n",
    "    #4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "        \n",
    "    #5. Return a list of words\n",
    "    return(words)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download the punkt tokenizer for sentence splitting\n",
    "import nltk.data\n",
    "# nltk.download\n",
    "\n",
    "# load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# define a func to split a review into parsed sentences\n",
    "def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    # Func to split a review into parsed sentences\n",
    "    # Returns a list of sentences, where each sentence is a list of words\n",
    "    \n",
    "    #1. use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    \n",
    "    #2. loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # if a sentence is empty, skip it\n",
    "        # Otherwise, call review_to_wordlist() to get a list of words\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(review_to_wordlist(raw_sentence, remove_stopwords))\n",
    "        \n",
    "        #Return the list of sentences(for a review, each sentence is a list of words. So this returns a list of lists)\n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n",
      "Parsing sentence from unlabeled set\n"
     ]
    }
   ],
   "source": [
    "#Now we can apply this function to prepare our data for input to Word2Vec\n",
    "sentences = []\n",
    "print(\"Parsing sentences from training set\")\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "\n",
    "print(\"Parsing sentence from unlabeled set\")\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75000\n",
      "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again']\n"
     ]
    }
   ],
   "source": [
    "# Check how many sentences we have in total - should be around 850,000+\n",
    "print(len(sentences))\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-23 19:50:14,787 : INFO : collecting all words and their counts\n",
      "2020-02-23 19:50:14,788 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-02-23 19:50:14,849 : INFO : PROGRESS: at sentence #10000, processed 250673 words, keeping 18555 word types\n",
      "2020-02-23 19:50:14,916 : INFO : PROGRESS: at sentence #20000, processed 496174 words, keeping 25579 word types\n",
      "2020-02-23 19:50:14,984 : INFO : PROGRESS: at sentence #30000, processed 748321 words, keeping 31383 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-23 19:50:15,050 : INFO : PROGRESS: at sentence #40000, processed 998433 words, keeping 36409 word types\n",
      "2020-02-23 19:50:15,120 : INFO : PROGRESS: at sentence #50000, processed 1252655 words, keeping 40516 word types\n",
      "2020-02-23 19:50:15,188 : INFO : PROGRESS: at sentence #60000, processed 1503721 words, keeping 43975 word types\n",
      "2020-02-23 19:50:15,251 : INFO : PROGRESS: at sentence #70000, processed 1755428 words, keeping 47041 word types\n",
      "2020-02-23 19:50:15,282 : INFO : collected 48555 word types from a corpus of 1882813 raw words and 75000 sentences\n",
      "2020-02-23 19:50:15,283 : INFO : Loading a fresh vocabulary\n",
      "2020-02-23 19:50:15,317 : INFO : effective_min_count=40 retains 3564 unique words (7% of original 48555, drops 44991)\n",
      "2020-02-23 19:50:15,318 : INFO : effective_min_count=40 leaves 1669156 word corpus (88% of original 1882813, drops 213657)\n",
      "2020-02-23 19:50:15,332 : INFO : deleting the raw counts dictionary of 48555 items\n",
      "2020-02-23 19:50:15,336 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2020-02-23 19:50:15,337 : INFO : downsampling leaves estimated 1163831 word corpus (69.7% of prior 1669156)\n",
      "2020-02-23 19:50:15,350 : INFO : estimated required memory for 3564 words and 300 dimensions: 10335600 bytes\n",
      "2020-02-23 19:50:15,352 : INFO : resetting layer weights\n",
      "2020-02-23 19:50:16,244 : INFO : training model with 4 workers on 3564 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2020-02-23 19:50:17,251 : INFO : EPOCH 1 - PROGRESS: at 74.75% examples, 868580 words/s, in_qsize 7, out_qsize 0\n",
      "2020-02-23 19:50:17,588 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-02-23 19:50:17,591 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-23 19:50:17,595 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-23 19:50:17,599 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-23 19:50:17,599 : INFO : EPOCH - 1 : training on 1882813 raw words (1163347 effective words) took 1.3s, 862512 effective words/s\n",
      "2020-02-23 19:50:18,612 : INFO : EPOCH 2 - PROGRESS: at 74.17% examples, 862376 words/s, in_qsize 7, out_qsize 0\n",
      "2020-02-23 19:50:18,946 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-02-23 19:50:18,952 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-23 19:50:18,957 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-23 19:50:18,959 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-23 19:50:18,959 : INFO : EPOCH - 2 : training on 1882813 raw words (1164436 effective words) took 1.3s, 862796 effective words/s\n",
      "2020-02-23 19:50:19,970 : INFO : EPOCH 3 - PROGRESS: at 77.97% examples, 905742 words/s, in_qsize 7, out_qsize 0\n",
      "2020-02-23 19:50:20,223 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-02-23 19:50:20,225 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-23 19:50:20,231 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-23 19:50:20,236 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-23 19:50:20,237 : INFO : EPOCH - 3 : training on 1882813 raw words (1163559 effective words) took 1.3s, 917634 effective words/s\n",
      "2020-02-23 19:50:21,248 : INFO : EPOCH 4 - PROGRESS: at 82.22% examples, 955053 words/s, in_qsize 7, out_qsize 0\n",
      "2020-02-23 19:50:21,449 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-02-23 19:50:21,460 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-23 19:50:21,464 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-23 19:50:21,466 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-23 19:50:21,467 : INFO : EPOCH - 4 : training on 1882813 raw words (1163504 effective words) took 1.2s, 954077 effective words/s\n",
      "2020-02-23 19:50:22,482 : INFO : EPOCH 5 - PROGRESS: at 80.12% examples, 927273 words/s, in_qsize 7, out_qsize 0\n",
      "2020-02-23 19:50:22,709 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-02-23 19:50:22,723 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-23 19:50:22,725 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-23 19:50:22,726 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-23 19:50:22,726 : INFO : EPOCH - 5 : training on 1882813 raw words (1164134 effective words) took 1.2s, 932490 effective words/s\n",
      "2020-02-23 19:50:22,727 : INFO : training on a 9414065 raw words (5818980 effective words) took 6.5s, 897652 effective words/s\n",
      "2020-02-23 19:50:22,727 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-02-23 19:50:22,740 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2020-02-23 19:50:22,741 : INFO : not storing attribute vectors_norm\n",
      "2020-02-23 19:50:22,742 : INFO : not storing attribute cum_table\n",
      "2020-02-23 19:50:22,840 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ysy/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  \n",
      "2020-02-23 19:50:51,624 : WARNING : vectors for words {'kitchen'} are not present in the model, ignoring these words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'child'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The \"doesnt_match\" function will try to deduce which word in a set is most dissimilar from the others:\n",
    "model.doesnt_match(\"man woman child kitchen\".split())\n",
    "# model.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ysy/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('doctor', 0.7730792760848999),\n",
       " ('priest', 0.7584525942802429),\n",
       " ('woman', 0.7580400705337524),\n",
       " ('scientist', 0.7402098774909973),\n",
       " ('cop', 0.7392619848251343),\n",
       " ('boy', 0.7265116572380066),\n",
       " ('himself', 0.717841386795044),\n",
       " ('girl', 0.7125262022018433),\n",
       " ('murderer', 0.7052661180496216),\n",
       " ('victim', 0.7049213647842407)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the \"most_similar\" function to get insight into the model's word clusters\n",
    "model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ysy/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('mary', 0.8550252914428711),\n",
       " ('joan', 0.8492039442062378),\n",
       " ('murderer', 0.8447268009185791),\n",
       " ('ruth', 0.8387153148651123),\n",
       " ('rose', 0.8385372161865234),\n",
       " ('singer', 0.8359367847442627),\n",
       " ('lady', 0.8326665163040161),\n",
       " ('former', 0.8248870372772217),\n",
       " ('boss', 0.823523223400116),\n",
       " ('mrs', 0.8226689696311951)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
